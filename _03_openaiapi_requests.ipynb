{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('code/')\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import functions\n",
    "\n",
    "# Working Dir.\n",
    "os.chdir('/Users/fogellmcmuffin/Documents/ra/team_discussions/AI/')\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-BBrbzGnsFwklndehaTC1T3BlbkFJEGPQt0QfhWkwp9ePuxaK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## Model Settings ##\n",
    "####################\n",
    "\n",
    "# Calling for OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY'),\n",
    "    organization='org-WLFAmqjnKmywM0wd6loMyGJq',    # RA_WORK\n",
    "    project='proj_vOr6WeeCFk5IjZLCdksFLWUd',    # IRPD_CODING\n",
    ")\n",
    "\n",
    "# Model Settings\n",
    "MODEL = 'gpt-4o-2024-05-13'\n",
    "TEMPERATURE = 0\n",
    "MAX_TOKENS = 1300\n",
    "TOP_P = 1\n",
    "FREQUENCY_PENALTY = 0\n",
    "PRESENCE_PENALTY = 0\n",
    "\n",
    "# Prompt request function\n",
    "def GPT_response(sys, user):\n",
    "  # Requesting chat completion\n",
    "  response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    frequency_penalty=FREQUENCY_PENALTY,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": str(sys)},\n",
    "      {\"role\": \"user\", \"content\": str(user)}\n",
    "    ]\n",
    "  )\n",
    "  output = response.choices[0].message.content  # GPT response var\n",
    "  \n",
    "  return output\n",
    "\n",
    "\n",
    "def test_info(test, data_name):  # Function to get test info\n",
    "  info = str(\n",
    "    'ChatGPT Model Information:' + '\\n' +\n",
    "    'format: OpenAI API' + '\\n' +\n",
    "    'model: ' + str(MODEL) + '\\n' +\n",
    "    'temperature: ' + str(TEMPERATURE) + '\\n' +\n",
    "    'max tokens: ' + str(MAX_TOKENS) + '\\n' +\n",
    "    'top p: ' + str(TOP_P) + '\\n' +\n",
    "    'frequency penalty: ' + str(FREQUENCY_PENALTY) + '\\n' +\n",
    "    'presence penalty: ' + str(PRESENCE_PENALTY) + '\\n\\n' +\n",
    "    'Test Information:' + '\\n' + \n",
    "    'test: ' + test + '\\n' +\n",
    "    'data: ' + data_name + '\\n' +\n",
    "    'date: ' + str(date.today()) + '\\n'\n",
    "  )\n",
    "  return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "## Test Functions ##\n",
    "####################\n",
    "\n",
    "def stage_1_output(treatment, test_type='test', max_windows=None, iterations=1):  # Stage 1 function: Creating categories\n",
    "  # Making test directory\n",
    "  if test_type == 'test':\n",
    "    test = functions.get_test_dir()\n",
    "    test_dir = os.path.join('output/', test)\n",
    "    info_path = os.path.join(test_dir, f't{test[5:]}_test_info.txt')  # Test info path\n",
    "  elif test_type == 'subtest':\n",
    "    test = functions.get_test_dir(test_type='subtest')\n",
    "    test_dir = os.path.join('output/_subtests/', test)\n",
    "    info_path = os.path.join(test_dir, f'{test}__subtest_info.txt')  # Test info path\n",
    "  \n",
    "  os.makedirs(test_dir, exist_ok=False)\n",
    "  \n",
    "  # Test info\n",
    "  version = functions.get_summary_version()\n",
    "  info = test_info(test=f\"Test {test[5:]}\" if test_type == 'test' else f\"Subtest {test}\", data_name=f'RAsum_{treatment}_ucoop_{version}.csv & RAsum_{treatment}_udef_{version}.csv')\n",
    "  functions.write_file(file_path=info_path, file_write=info)\n",
    "  \n",
    "  for i in ['ucoop', 'udef']:\n",
    "    iteration_count = 0\n",
    "    \n",
    "    inst_dir = os.path.join(test_dir, f'stage_1_{i}') # Creating ind. instance directory\n",
    "    os.makedirs(inst_dir, exist_ok=False)\n",
    "    \n",
    "    # Iterating process\n",
    "    while iteration_count < iterations:\n",
    "      ## Step 1: Generate category using set of summaries ##\n",
    "      # System prompt\n",
    "      sys_1 = functions.file_to_string(file_path=f'prompts/approach_2/stage_1/sys_1.md')\n",
    "    \n",
    "      # Summary data (User prompts)\n",
    "      if iteration_count < 1:\n",
    "        df = pd.read_csv(f'test_data/RAsum_{treatment}_{i}_v{version}.csv')\n",
    "        df = df[:max_windows]\n",
    "      else:\n",
    "        previous_cat = functions.get_cat_number(stage_dir=inst_dir, previous=True)\n",
    "        previous_cat_dir = os.path.join(inst_dir, previous_cat)\n",
    "        previous_cat_out = os.path.join(previous_cat_dir, f't{test[5:]}_stg_1_{previous_cat}_{i}_output.csv' if test_type == 'test' else f'{test}_stg_1_{previous_cat}_{i}_output.csv')\n",
    "        df = pd.read_csv(previous_cat_out)\n",
    "        df = df.loc[df['belongs'] == 0]\n",
    "        df = df.drop(['belongs', 'response'], axis=1)\n",
    "      \n",
    "      cat_number = functions.get_cat_number(stage_dir=inst_dir, previous=False)\n",
    "      cat_dir = os.path.join(inst_dir, cat_number)\n",
    "      os.makedirs(cat_dir, exist_ok=False)\n",
    "      \n",
    "      df['window_number'] = df['window_number'].astype(int)   # Making sure window number is an integer\n",
    "      user_1 = str(df.to_dict('records'))\n",
    "      \n",
    "      # GPT request output\n",
    "      output_1 = GPT_response(sys=sys_1, user=user_1)\n",
    "      \n",
    "      ## Step 2: Determine immediate classifcations\n",
    "      # System prompt\n",
    "      sys_prmpt = functions.file_to_string(file_path=f'prompts/approach_2/stage_1/sys_2_{treatment}_{i}.md')\n",
    "      sys_2 = sys_prmpt + '\\n' + str(output_1)\n",
    "      \n",
    "      # GPT request output\n",
    "      output_df = pd.DataFrame(df)\n",
    "      output_df['belongs'] = 0\n",
    "      for k in range(len(df)):\n",
    "        row = df.iloc[k].to_dict()  # Creating a dictionary for each indv. row\n",
    "      \n",
    "        output_2 = GPT_response(sys_2, str(row))  # GPT request output\n",
    "        output_df.loc[k, 'response'] = output_2\n",
    "        output_df.loc[k, 'belongs'] = 1 if \"yes\" in output_2.lower() else 0\n",
    "        \n",
    "      \n",
    "      # Creating paths for prompts & GPT response\n",
    "      if test_type == 'test':\n",
    "        sys_1_prmpt_path = os.path.join(cat_dir, f't{test[5:]}_stg_1_{cat_number}_{i}_sys1_prmpt.txt')\n",
    "        sys_2_prmpt_path = os.path.join(cat_dir, f't{test[5:]}_stg_1_{cat_number}_{i}_sys2_prmpt.txt')\n",
    "        user_prmpt_path = os.path.join(cat_dir, f't{test[5:]}_stg_1_{cat_number}_{i}_user_prmpt.txt')\n",
    "        response_path = os.path.join(cat_dir, f't{test[5:]}_stg_1_{cat_number}_{i}_response.txt')\n",
    "        output_path = os.path.join(cat_dir, f't{test[5:]}_stg_1_{cat_number}_{i}_output.csv')\n",
    "      elif test_type == 'subtest':\n",
    "        sys_1_prmpt_path = os.path.join(cat_dir, f'{test}_stg_1_{cat_number}_{i}_sys1_prmpt.txt')\n",
    "        sys_2_prmpt_path = os.path.join(cat_dir, f'{test}_stg_1_{cat_number}_{i}_sys2_prmpt.txt')\n",
    "        user_prmpt_path = os.path.join(cat_dir, f'{test}_stg_1_{cat_number}_{i}_user_prmpt.txt')\n",
    "        response_path = os.path.join(cat_dir, f'{test}_stg_1_{cat_number}_{i}_response.txt')\n",
    "        output_path = os.path.join(cat_dir, f'{test}_stg_1_{cat_number}_{i}_output.csv')\n",
    "      \n",
    "      # Writing .txt files for prompts & GPT response\n",
    "      functions.write_file(file_path=sys_1_prmpt_path, file_write=sys_1)\n",
    "      functions.write_file(file_path=sys_2_prmpt_path, file_write=sys_2)\n",
    "      functions.write_file(file_path=user_prmpt_path, file_write=user_1)\n",
    "      functions.write_file(file_path=response_path, file_write=str(output_1))\n",
    "      output_df.to_csv(output_path, index=False)\n",
    "      iteration_count += 1\n",
    "  \n",
    "  return print(\"Stage 1 Complete\")\n",
    "\n",
    "\n",
    "def stage_2_output(treatment, max_windows=None, test_type='test'): # Stage 2 function: Assigning categories to individual summaries (recursive)\n",
    "  # Getting test directory\n",
    "  if test_type == 'test':\n",
    "    test = get_test_dir(cycle=True)\n",
    "    test_dir = os.path.join('output/', test)\n",
    "  elif test_type == 'subtest':\n",
    "    test = get_test_dir(test_type='subtest', cycle=True)\n",
    "    test_dir = os.path.join('output/_subtests/', test)\n",
    "  \n",
    "  # System Prompts\n",
    "  stg_2_ucoop_sys = file_to_string(file_path=f'prompts/ucoop/{treatment}/sys_2_{treatment}_ucoop.md') # Getting system prompts for stage 2\n",
    "  stg_2_udef_sys = file_to_string(file_path=f'prompts/udef/{treatment}/sys_2_{treatment}_udef.md')\n",
    "  \n",
    "  ## Getting Stage 1 response to merge with Stage 2 system prompt\n",
    "  stg_1_ucoop_dir = os.path.join(test_dir, 'stage_1_ucoop/')\n",
    "  stg_1_udef_dir = os.path.join(test_dir, 'stage_1_udef/')\n",
    "  if test_type == 'test':\n",
    "    ucoop_response_path = os.path.join(stg_1_ucoop_dir, f't{test[5:]}_stg_1_ucoop_response.txt')\n",
    "    udef_response_path = os.path.join(stg_1_udef_dir, f't{test[5:]}_stg_1_udef_response.txt')\n",
    "  elif test_type == 'subtest':\n",
    "    ucoop_response_path = os.path.join(stg_1_ucoop_dir, f'{test}_stg_1_ucoop_response.txt')\n",
    "    udef_response_path = os.path.join(stg_1_udef_dir, f'{test}_stg_1_udef_response.txt')\n",
    "  \n",
    "  ucoop_response = file_to_string(file_path=ucoop_response_path)\n",
    "  udef_response = file_to_string(file_path=udef_response_path)\n",
    "  \n",
    "  sys_ucoop = stg_2_ucoop_sys + '\\n' + ucoop_response   # Final system prompts\n",
    "  sys_udef = stg_2_udef_sys + '\\n' + udef_response\n",
    "  \n",
    "  # Summary data (User prompts)\n",
    "  version = get_summary_version()\n",
    "  df_ucoop = pd.read_csv(f'test_data/RAsum_{treatment}_ucoop_{version}.csv')\n",
    "  df_udef = pd.read_csv(f'test_data/RAsum_{treatment}_udef_{version}.csv')\n",
    "  \n",
    "  df_ucoop['window_number'] = df_ucoop['window_number'].astype(int)   # Making sure window number is an integer\n",
    "  df_udef['window_number'] = df_udef['window_number'].astype(int)\n",
    "  \n",
    "  df_ucoop = df_ucoop[:max_windows] if max_windows != None else df_ucoop  # Adjusting to max windows for Stage 2\n",
    "  df_udef = df_udef[:max_windows] if max_windows != None else df_udef\n",
    "  \n",
    "  # Aggregating prompts\n",
    "  window_prompts = [['ucoop', sys_ucoop, df_ucoop], ['udef', sys_udef, df_udef]]\n",
    "  \n",
    "  for i in window_prompts:\n",
    "    # Requests for both ucoop and udef instances\n",
    "    inst_dir = os.path.join(test_dir, f'stage_2_{i[0]}') # Creating ind. instance directory\n",
    "    os.makedirs(inst_dir, exist_ok=False)\n",
    "    \n",
    "    sys_prmpt = i[1]    # System prompt for ucoop or udef data\n",
    "    sys_prmpt_path = os.path.join(inst_dir, f't{test[5:]}_stg_2_{i[0]}_sys_prmpt.txt') if test_type == 'test' else os.path.join(inst_dir, f'{test}_stg_2_{i[0]}_sys_prmpt.txt')\n",
    "    write_file(file_path=sys_prmpt_path, file_write=sys_prmpt)\n",
    "    \n",
    "    # Prompt & Response paths\n",
    "    prompt_path = os.path.join(inst_dir, 'prompts')\n",
    "    response_path = os.path.join(inst_dir, 'responses')\n",
    "    os.makedirs(prompt_path, exist_ok=True)\n",
    "    os.makedirs(response_path, exist_ok=True)\n",
    "    \n",
    "    # Requesting chat completion for each row\n",
    "    df = i[2]   # Test data for ucoop or udef data\n",
    "    for k in range(len(df)):\n",
    "      row = df.iloc[k].to_dict()  # Creating a dictionary for each indv. row\n",
    "      \n",
    "      output = GPT_response(sys_prmpt, str(row))  # GPT request output\n",
    "      \n",
    "      # Creating paths for prompts & GPT responses using window_numbers\n",
    "      window_number = row['window_number']\n",
    "      \n",
    "      if test_type == 'test':\n",
    "        user_prmpt_path = os.path.join(prompt_path, f't{test[5:]}_{window_number}_user_prmpt.txt')\n",
    "        output_path = os.path.join(response_path, f't{test[5:]}_{window_number}_response.txt')\n",
    "      elif test_type == 'subtest':\n",
    "        user_prmpt_path = os.path.join(prompt_path, f'{test}_{window_number}_user_prmpt.txt')\n",
    "        output_path = os.path.join(response_path, f'{test}_{window_number}_response.txt')\n",
    "      \n",
    "      # Writing .txt files for prompts & GPT response\n",
    "      write_file(user_prmpt_path, str(row))\n",
    "      write_file(output_path, str(output))\n",
    "    \n",
    "    # Prelimaries to final output\n",
    "    if i[0] == 'ucoop':\n",
    "      df['unilateral_cooperation'] = 1\n",
    "      ucoop_df = response_df(response_dir=response_path, test_df=df)  # Coding GPT classifications for ucoop instances\n",
    "      ucoop_df = ucoop_udef_rename(ucoop_df, 'ucoop') # Adding ucoop prefix to categories\n",
    "    else:\n",
    "      df['unilateral_cooperation'] = 0\n",
    "      udef_df = response_df(response_dir=response_path, test_df=df)   # Coding GPT classifications for udef instances\n",
    "      udef_df = ucoop_udef_rename(udef_df, 'udef')    # Adding udef prefix to categories\n",
    "  \n",
    "  # Final output dataframe\n",
    "  GPT_df = pd.concat([ucoop_df, udef_df], ignore_index=True, sort=False)\n",
    "  GPT_df = GPT_df.fillna(0)\n",
    "  final_out_path = os.path.join(test_dir, f\"t{test[5:]}_final_output.csv\" if test_type == 'test' else f\"{test}_final_output.csv\")\n",
    "  GPT_df.to_csv(final_out_path, index=False)\n",
    "  \n",
    "  return print(\"Stage 2 Complete\")\n",
    "\n",
    "\n",
    "def full_test_output(treatment, test_type='test', max_windows=None):\n",
    "  # Running Stage 1\n",
    "  stage_1_output(treatment=treatment, test_type=test_type)\n",
    "  \n",
    "  # Running Stage 2\n",
    "  stage_2_output(treatment=treatment, test_type=test_type, max_windows=max_windows)\n",
    "  \n",
    "  return print(\"Test Complete\")\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "## Requests ##\n",
    "##############\n",
    "\n",
    "# Stage 1\n",
    "stage_1_output(treatment='noise', test_type='subtest', max_windows=10, iterations=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2\n",
    "stage_2_output(treatment='noise', test_type='subtest', max_windows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
